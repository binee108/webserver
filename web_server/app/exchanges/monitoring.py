#!/usr/bin/env python3
"""
Exchange System ëª¨ë‹ˆí„°ë§ ë° ê²€ì¦ ì‹œìŠ¤í…œ

Enhanced Exchangeì™€ Legacy CCXTì˜ ì„±ëŠ¥, ì •í™•ì„±, ì•ˆì •ì„±ì„ ì§€ì†ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ë©”íŠ¸ë¦­
- ë°ì´í„° ì •í™•ì„± ê²€ì¦ 
- ìë™ ê²½ê³  ë° ì•Œë¦¼
- ëŒ€ì‹œë³´ë“œìš© ë©”íŠ¸ë¦­ ìˆ˜ì§‘
"""

import time
import json
import logging
import asyncio
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from collections import defaultdict, deque
from threading import Lock
import statistics

from .enhanced_factory import enhanced_factory
# Migration system removed - using simplified configuration
from .registry import exchange_registry

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetric:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    timestamp: datetime
    exchange: str
    function: str
    is_enhanced: bool
    response_time_ms: float
    success: bool
    error_type: Optional[str] = None
    user_id: Optional[str] = None

@dataclass
class AccuracyTest:
    """ì •í™•ì„± í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    timestamp: datetime
    exchange: str
    function: str
    symbol: str
    enhanced_result: Any
    legacy_result: Any
    is_identical: bool
    differences: List[str]
    
class MetricsCollector:
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, max_history: int = 10000):
        self.max_history = max_history
        self._metrics: deque = deque(maxlen=max_history)
        self._accuracy_tests: deque = deque(maxlen=max_history)
        self._lock = Lock()
        
        # ì‹¤ì‹œê°„ í†µê³„ìš© ë²„í¼
        self._recent_metrics = defaultdict(lambda: deque(maxlen=100))
        
        logger.info("ğŸ“Š MetricsCollector ì´ˆê¸°í™” ì™„ë£Œ")
    
    def record_performance(
        self, 
        exchange: str,
        function: str,
        is_enhanced: bool,
        response_time_ms: float,
        success: bool,
        error_type: str = None,
        user_id: str = None
    ):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        metric = PerformanceMetric(
            timestamp=datetime.now(),
            exchange=exchange,
            function=function,
            is_enhanced=is_enhanced,
            response_time_ms=response_time_ms,
            success=success,
            error_type=error_type,
            user_id=user_id
        )
        
        with self._lock:
            self._metrics.append(metric)
            
            # ì‹¤ì‹œê°„ í†µê³„ìš© ë²„í¼ ì—…ë°ì´íŠ¸
            key = f"{exchange}_{function}_{is_enhanced}"
            self._recent_metrics[key].append(metric)
        
        # Migration system removed - performance recorded locally only
        
        # ê²½ê³  ì„ê³„ê°’ í™•ì¸
        self._check_performance_alerts(metric)
    
    def record_accuracy_test(
        self,
        exchange: str,
        function: str, 
        symbol: str,
        enhanced_result: Any,
        legacy_result: Any
    ):
        """ì •í™•ì„± í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡"""
        differences = []
        is_identical = self._compare_results(enhanced_result, legacy_result, differences)
        
        test = AccuracyTest(
            timestamp=datetime.now(),
            exchange=exchange,
            function=function,
            symbol=symbol,
            enhanced_result=enhanced_result,
            legacy_result=legacy_result,
            is_identical=is_identical,
            differences=differences
        )
        
        with self._lock:
            self._accuracy_tests.append(test)
        
        if not is_identical:
            logger.warning(f"âš ï¸ ì •í™•ì„± ì°¨ì´ ë°œê²¬: {exchange} {function} {symbol} - {len(differences)}ê°œ ì°¨ì´ì ")
            for diff in differences[:3]:  # ì²˜ìŒ 3ê°œë§Œ ë¡œê·¸
                logger.warning(f"   - {diff}")
    
    def _compare_results(self, enhanced: Any, legacy: Any, differences: List[str]) -> bool:
        """ê²°ê³¼ ë¹„êµ (ì¬ê·€ì )"""
        try:
            if type(enhanced) != type(legacy):
                differences.append(f"Type mismatch: {type(enhanced).__name__} vs {type(legacy).__name__}")
                return False
            
            if isinstance(enhanced, dict):
                enhanced_keys = set(enhanced.keys())
                legacy_keys = set(legacy.keys())
                
                if enhanced_keys != legacy_keys:
                    missing_in_enhanced = legacy_keys - enhanced_keys
                    missing_in_legacy = enhanced_keys - legacy_keys
                    if missing_in_enhanced:
                        differences.append(f"Missing in enhanced: {missing_in_enhanced}")
                    if missing_in_legacy:
                        differences.append(f"Missing in legacy: {missing_in_legacy}")
                
                is_identical = True
                for key in enhanced_keys & legacy_keys:
                    if not self._compare_results(enhanced[key], legacy[key], differences):
                        is_identical = False
                        if len(differences) > 10:  # ë„ˆë¬´ ë§ì€ ì°¨ì´ì ì€ ì œí•œ
                            break
                
                return is_identical and enhanced_keys == legacy_keys
            
            elif isinstance(enhanced, (list, tuple)):
                if len(enhanced) != len(legacy):
                    differences.append(f"Length mismatch: {len(enhanced)} vs {len(legacy)}")
                    return False
                
                for i, (e_item, l_item) in enumerate(zip(enhanced, legacy)):
                    if not self._compare_results(e_item, l_item, differences):
                        differences.append(f"Index {i} differs")
                        if len(differences) > 10:
                            break
                
                return len(differences) == 0
            
            elif isinstance(enhanced, float) and isinstance(legacy, float):
                # ë¶€ë™ì†Œìˆ˜ì  ë¹„êµ (ì‘ì€ ì˜¤ì°¨ í—ˆìš©)
                if abs(enhanced - legacy) > 1e-8:
                    differences.append(f"Float precision: {enhanced} vs {legacy}")
                    return False
                return True
            
            else:
                if enhanced != legacy:
                    differences.append(f"Value mismatch: {enhanced} vs {legacy}")
                    return False
                return True
                
        except Exception as e:
            differences.append(f"Comparison error: {e}")
            return False
    
    def _check_performance_alerts(self, metric: PerformanceMetric):
        """ì„±ëŠ¥ ê²½ê³  í™•ì¸"""
        # ì‘ë‹µ ì‹œê°„ ê²½ê³  (2ì´ˆ ì´ˆê³¼)
        if metric.response_time_ms > 2000:
            logger.warning(f"ğŸŒ ëŠë¦° ì‘ë‹µ: {metric.exchange} {metric.function} ({metric.response_time_ms:.1f}ms)")
        
        # ì˜¤ë¥˜ ê²½ê³ 
        if not metric.success:
            logger.error(f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {metric.exchange} {metric.function} - {metric.error_type}")
    
    def get_performance_summary(self, hours: int = 24) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ í†µê³„"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        with self._lock:
            recent_metrics = [m for m in self._metrics if m.timestamp > cutoff_time]
        
        if not recent_metrics:
            return {'total_requests': 0, 'message': 'No recent data'}
        
        # ê¸°ë³¸ í†µê³„
        total_requests = len(recent_metrics)
        enhanced_requests = len([m for m in recent_metrics if m.is_enhanced])
        successful_requests = len([m for m in recent_metrics if m.success])
        
        # ì„±ëŠ¥ í†µê³„
        response_times = [m.response_time_ms for m in recent_metrics]
        
        # ê±°ë˜ì†Œë³„ í†µê³„
        exchange_stats = defaultdict(lambda: {'total': 0, 'enhanced': 0, 'success': 0, 'response_times': []})
        for metric in recent_metrics:
            stats = exchange_stats[metric.exchange]
            stats['total'] += 1
            if metric.is_enhanced:
                stats['enhanced'] += 1
            if metric.success:
                stats['success'] += 1
            stats['response_times'].append(metric.response_time_ms)
        
        return {
            'period_hours': hours,
            'total_requests': total_requests,
            'enhanced_usage_percentage': (enhanced_requests / total_requests * 100) if total_requests > 0 else 0,
            'success_rate_percentage': (successful_requests / total_requests * 100) if total_requests > 0 else 0,
            'performance': {
                'avg_response_time_ms': statistics.mean(response_times) if response_times else 0,
                'median_response_time_ms': statistics.median(response_times) if response_times else 0,
                'p95_response_time_ms': statistics.quantiles(response_times, n=20)[18] if len(response_times) > 20 else 0,
                'max_response_time_ms': max(response_times) if response_times else 0
            },
            'exchanges': {
                name: {
                    'total_requests': stats['total'],
                    'enhanced_percentage': (stats['enhanced'] / stats['total'] * 100) if stats['total'] > 0 else 0,
                    'success_rate': (stats['success'] / stats['total'] * 100) if stats['total'] > 0 else 0,
                    'avg_response_time_ms': statistics.mean(stats['response_times']) if stats['response_times'] else 0
                }
                for name, stats in exchange_stats.items()
            }
        }
    
    def get_accuracy_summary(self, hours: int = 24) -> Dict[str, Any]:
        """ì •í™•ì„± ìš”ì•½ í†µê³„"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        with self._lock:
            recent_tests = [t for t in self._accuracy_tests if t.timestamp > cutoff_time]
        
        if not recent_tests:
            return {'total_tests': 0, 'message': 'No recent accuracy tests'}
        
        total_tests = len(recent_tests)
        identical_results = len([t for t in recent_tests if t.is_identical])
        
        # ê¸°ëŠ¥ë³„ ì •í™•ì„±
        function_stats = defaultdict(lambda: {'total': 0, 'identical': 0, 'differences': []})
        for test in recent_tests:
            stats = function_stats[test.function]
            stats['total'] += 1
            if test.is_identical:
                stats['identical'] += 1
            else:
                stats['differences'].extend(test.differences[:3])  # ì²˜ìŒ 3ê°œë§Œ
        
        return {
            'period_hours': hours,
            'total_tests': total_tests,
            'accuracy_percentage': (identical_results / total_tests * 100) if total_tests > 0 else 0,
            'identical_results': identical_results,
            'different_results': total_tests - identical_results,
            'functions': {
                name: {
                    'total_tests': stats['total'],
                    'accuracy_percentage': (stats['identical'] / stats['total'] * 100) if stats['total'] > 0 else 0,
                    'common_differences': list(set(stats['differences'][:5]))  # ìƒìœ„ 5ê°œ ì°¨ì´ì 
                }
                for name, stats in function_stats.items()
            }
        }

class SystemMonitor:
    """ì „ì²´ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self._monitoring_active = False
        self._monitoring_task = None
        
        logger.info("ğŸ” SystemMonitor ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def start_monitoring(self, interval_seconds: int = 60):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self._monitoring_active:
            logger.warning("âš ï¸ ëª¨ë‹ˆí„°ë§ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
            return
        
        self._monitoring_active = True
        self._monitoring_task = asyncio.create_task(self._monitoring_loop(interval_seconds))
        logger.info(f"ğŸš€ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ê°„ê²©: {interval_seconds}ì´ˆ)")
    
    async def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        if not self._monitoring_active:
            return
        
        self._monitoring_active = False
        if self._monitoring_task:
            self._monitoring_task.cancel()
            try:
                await self._monitoring_task
            except asyncio.CancelledError:
                pass
        
        logger.info("â¹ï¸ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    async def _monitoring_loop(self, interval_seconds: int):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self._monitoring_active:
            try:
                await self._collect_system_metrics()
                await asyncio.sleep(interval_seconds)
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(interval_seconds)
    
    async def _collect_system_metrics(self):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            # Enhanced Factory í†µê³„
            factory_stats = enhanced_factory.get_creation_stats()
            
            # Registry í†µê³„
            registry_stats = exchange_registry.get_stats()
            
            # ì„±ëŠ¥ ìš”ì•½
            performance_summary = self.metrics_collector.get_performance_summary(hours=1)
            
            # ê²½ê³  ì¡°ê±´ í™•ì¸
            await self._check_system_health(performance_summary, factory_stats)
            
        except Exception as e:
            logger.error(f"âŒ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    async def _check_system_health(self, performance: Dict, factory: Dict):
        """ì‹œìŠ¤í…œ ê±´ê°•ì„± í™•ì¸"""
        warnings = []
        
        # ì„±ê³µë¥  í™•ì¸
        if performance.get('success_rate_percentage', 100) < 95:
            warnings.append(f"ë‚®ì€ ì„±ê³µë¥ : {performance['success_rate_percentage']:.1f}%")
        
        # ì‘ë‹µ ì‹œê°„ í™•ì¸
        avg_response = performance.get('performance', {}).get('avg_response_time_ms', 0)
        if avg_response > 1000:
            warnings.append(f"ëŠë¦° ì‘ë‹µì‹œê°„: {avg_response:.1f}ms")
        
        # Factory ì˜¤ë¥˜ í™•ì¸
        if factory.get('creation_errors', 0) > 10:
            warnings.append(f"Factory ì˜¤ë¥˜ ë‹¤ë°œ: {factory['creation_errors']}ê±´")
        
        # ê²½ê³  ë¡œê¹…
        if warnings:
            logger.warning(f"âš ï¸ ì‹œìŠ¤í…œ ê±´ê°•ì„± ê²½ê³ : {', '.join(warnings)}")
        
        # ê¸´ê¸‰ ìƒí™© í™•ì¸
        if (performance.get('success_rate_percentage', 100) < 90 and 
            performance.get('total_requests', 0) > 100):
            logger.critical("ğŸš¨ ê¸´ê¸‰ìƒí™©: ì„±ê³µë¥  90% ë¯¸ë§Œ, ê¸´ê¸‰ ë¡¤ë°± ê³ ë ¤")
    
    def get_dashboard_data(self) -> Dict[str, Any]:
        """ëŒ€ì‹œë³´ë“œìš© ë°ì´í„°"""
        return {
            'timestamp': datetime.now().isoformat(),
            'performance': self.metrics_collector.get_performance_summary(),
            'accuracy': self.metrics_collector.get_accuracy_summary(),
            # Migration system removed - using simplified configuration
            'factory': enhanced_factory.get_creation_stats(),
            'registry': exchange_registry.get_stats()
        }

# ì „ì—­ ëª¨ë‹ˆí„°ë§ ì¸ìŠ¤í„´ìŠ¤
system_monitor = SystemMonitor()

# ë°ì½”ë ˆì´í„° í•¨ìˆ˜ë“¤
def monitor_performance(exchange: str, function: str, is_enhanced: bool = False):
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        if asyncio.iscoroutinefunction(func):
            async def async_wrapper(*args, **kwargs):
                start_time = time.time()
                success = False
                error_type = None
                
                try:
                    result = await func(*args, **kwargs)
                    success = True
                    return result
                except Exception as e:
                    error_type = type(e).__name__
                    raise
                finally:
                    response_time_ms = (time.time() - start_time) * 1000
                    system_monitor.metrics_collector.record_performance(
                        exchange=exchange,
                        function=function,
                        is_enhanced=is_enhanced,
                        response_time_ms=response_time_ms,
                        success=success,
                        error_type=error_type
                    )
            return async_wrapper
        else:
            def sync_wrapper(*args, **kwargs):
                start_time = time.time()
                success = False
                error_type = None
                
                try:
                    result = func(*args, **kwargs)
                    success = True
                    return result
                except Exception as e:
                    error_type = type(e).__name__
                    raise
                finally:
                    response_time_ms = (time.time() - start_time) * 1000
                    system_monitor.metrics_collector.record_performance(
                        exchange=exchange,
                        function=function,
                        is_enhanced=is_enhanced,
                        response_time_ms=response_time_ms,
                        success=success,
                        error_type=error_type
                    )
            return sync_wrapper
    return decorator

# í¸ì˜ í•¨ìˆ˜ë“¤
def record_performance_metric(
    exchange: str,
    function: str, 
    is_enhanced: bool,
    response_time_ms: float,
    success: bool,
    error_type: str = None
):
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡ (ì „ì—­ í•¨ìˆ˜)"""
    system_monitor.metrics_collector.record_performance(
        exchange=exchange,
        function=function,
        is_enhanced=is_enhanced,
        response_time_ms=response_time_ms,
        success=success,
        error_type=error_type
    )

def record_accuracy_test(
    exchange: str,
    function: str,
    symbol: str,
    enhanced_result: Any,
    legacy_result: Any
):
    """ì •í™•ì„± í…ŒìŠ¤íŠ¸ ê¸°ë¡ (ì „ì—­ í•¨ìˆ˜)"""
    system_monitor.metrics_collector.record_accuracy_test(
        exchange=exchange,
        function=function,
        symbol=symbol,
        enhanced_result=enhanced_result,
        legacy_result=legacy_result
    )